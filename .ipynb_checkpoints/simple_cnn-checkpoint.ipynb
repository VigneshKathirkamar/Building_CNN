{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the necessary libraries\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(10000, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "# mnist data comes pre-build in tensorflow, we are loading\n",
    "# the data and splitting it into test and train\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "# By printing the shape of our dataset we can find that \n",
    "# X train contains 60000 images of 28x28 pixel and y_train\n",
    "# contains 60000 labels. Similarly, X_test and y_test contains\n",
    "# the image and labels respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell we are reshaping the data to fit the \n",
    "# input of neural networkd\n",
    "X_train = X_train.reshape(60000,28,28,1)\n",
    "X_test = X_test.reshape(10000,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# In this column we do one-hot encoding which means, if our label is 3\n",
    "# then corresponding label will be [0,0,0,1,0,0,0,0,0,0]\n",
    "# for 4 the label will be [0,0,0,0,4,0,0,0]\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# one-hot encode target column\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "print(y_train[0]) #visulaizing the first element of y_train after one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the first convolution layer\n",
    "model.add(Conv2D(filters=70,kernel_size=3,activation='relu',input_shape=(28,28,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second convolution layer\n",
    "model.add(Conv2D(filters=64,kernel_size=3,activation='relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After convolving two times we flatten our input such that it is \n",
    "# vector with size 1x784 (28x28)\n",
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell we use softmax as our activation function\n",
    "model.add(Dense(10,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adagrad',metrics=['accuracy'],loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 56s 30ms/step - loss: 0.2814 - accuracy: 0.9441 - val_loss: 0.0940 - val_accuracy: 0.9720\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 56s 30ms/step - loss: 0.0585 - accuracy: 0.9820 - val_loss: 0.0740 - val_accuracy: 0.9777\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 56s 30ms/step - loss: 0.0356 - accuracy: 0.9901 - val_loss: 0.0659 - val_accuracy: 0.9808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f26c6bfafd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',metrics=['accuracy'],loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1875/1875 [==============================] - 56s 30ms/step - loss: 0.1715 - accuracy: 0.9612 - val_loss: 0.0946 - val_accuracy: 0.9753\n",
      "Epoch 2/3\n",
      "1875/1875 [==============================] - 56s 30ms/step - loss: 0.0824 - accuracy: 0.9787 - val_loss: 0.0764 - val_accuracy: 0.9781\n",
      "Epoch 3/3\n",
      "1875/1875 [==============================] - 56s 30ms/step - loss: 0.0751 - accuracy: 0.9803 - val_loss: 0.1102 - val_accuracy: 0.9735\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f26bc0558d0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('number_predictor.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional\n",
    "# converting tensorlfow model into tflite model\n",
    "tf_lite_converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tf_lite_model = tf_lite_converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_model_name = 'number_predictor.tflite'\n",
    "open(tflite_model_name,\"wb\").write(tf_lite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpreting our tflite model\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path='number_predictor.tflite')\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_input_details()\n",
    "print(\"input shape: \",input_details[0]['shape'])\n",
    "print(\"Input type: \",input_details[0]['dtype'])\n",
    "print(\"Output shape: \",output_details[0]['shape'])\n",
    "print(\"output type: \",output_details[0]['dtype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAN70lEQVR4nO3df4gd9bnH8c9jTBBikdX1hiVNr20SIYlgellFvUGMP6JVMPYPSyNccr3B7R8VWlC4Yv+IUK6Uq23xr+AWJbmlTSio11AuthqCqaIlmx+a1dw2XtmQrDFRTIgNslHz3D/OpKxx5zubMzNnJj7vFyx7zjyZM0+O+Thz5ntmvubuAvDVd17TDQDoDcIOBEHYgSAIOxAEYQeCOL+XGzMzTv0DNXN3m2p5qT27md1mZn8xs3fM7KEyrwWgXtbtOLuZzZD0V0m3SDooabukVe7+dmId9uxAzerYs18t6R13f9fdT0raJGllidcDUKMyYZ8r6cCk5wezZV9gZkNmNmJmIyW2BaCk2k/QufuwpGGJw3igSWX27OOS5k16/vVsGYAWKhP27ZIWmtk3zWyWpO9L2lxNWwCq1vVhvLt/Zmb3S/qDpBmSnnb3tyrrDECluh5662pjfGYHalfLl2oAnDsIOxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0Ioqe3kkZ3LrjggmR9xYoVubU777wzue5VV12VrI+Ojibrjz76aLI+NjaWWztx4kRyXVSLPTsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBMHdZVvgwQcfTNYXLVqUrK9evbrKds6K2ZQ3Mv273bt359aGhoaS6+7YsaOblsLj7rJAcIQdCIKwA0EQdiAIwg4EQdiBIAg7EATj7C2wZ8+eZH3x4sXJei//G56paJw91dv4+Hhy3csvvzxZn5iYSNajyhtnL3XzCjMbk/SxpM8lfebug2VeD0B9qrhTzXJ3/7CC1wFQIz6zA0GUDbtL+qOZ7TCzKb/obGZDZjZiZiMltwWghLKH8cvcfdzM/kHSi2b2v+6+bfIfcPdhScMSJ+iAJpXas7v7ePb7iKTnJF1dRVMAqtd12M1stpl97fRjSSskpe87DKAxZQ7j50h6LhtnPV/Sb939hUq6wjnjvffeS9YHBgZya3Pnzk2u+9JLLyXrq1atStYPHjyYrEfTddjd/V1JV1bYC4AaMfQGBEHYgSAIOxAEYQeCIOxAEEzZHFzR0FnR8NbRo0eT9ccffzy3lppqWpKuvfbaZP2+++5L1teuXZusR8OeHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeC4FbSLVB0K+klS5Yk62NjY7m1xx57LLnuunXrkvWyFixYkFsruoR13rx5pba9fPny3Nq2bdtya+c6pmwGgiPsQBCEHQiCsANBEHYgCMIOBEHYgSAYZ2+BwcH05LezZ89O1t94443c2rFjx7ppqSeWLl2arI+MlJsx7IknnsitPfDAA6Veu80YZweCI+xAEIQdCIKwA0EQdiAIwg4EQdiBIBhnR2OKvj/w2muvJeuLFy9O1vft25dbu/LK9ATEJ0+eTNbbrOtxdjN72syOmNnopGUXm9mLZrYv+91XZbMAqjedw/j1km47Y9lDkra4+0JJW7LnAFqsMOzuvk3SR2csXilpQ/Z4g6S7qm0LQNW6nettjrsfyh6/L2lO3h80syFJQ11uB0BFSk/s6O6eOvHm7sOShiVO0AFN6nbo7bCZDUhS9vtIdS0BqEO3Yd8saXX2eLWk56tpB0BdCsfZzWyjpBsk9Us6LGmtpP+W9DtJ35C0X9L33P3Mk3hTvRaH8Zi2NWvWJOtPPvlksm425XCzJOnGG29Mrvvyyy8n622WN85e+Jnd3VfllG4q1RGAnuLrskAQhB0IgrADQRB2IAjCDgTBJa5oraJLYHft2pWsz58/P7cWceiNPTsQBGEHgiDsQBCEHQiCsANBEHYgCMIOBFH6TjVAXU6cOJGsT0xMJOvnnce+bDLeDSAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgnH2c8BFF12UrK9YsaJHnZy9AwcO5NZef/31Wrd96tSp3NoVV1yRXPdcvp49D3t2IAjCDgRB2IEgCDsQBGEHgiDsQBCEHQiCcfYKFF03fc899yTrqfubS9Ly5cuT9WXLliXrdUpNiyylrzk/fvx4qW339fV1ve7o6GipbZ+LCvfsZva0mR0xs9FJyx4xs3Ez25393F5vmwDKms5h/HpJt02x/JfuvjT7+Z9q2wJQtcKwu/s2SR/1oBcANSpzgu5+M3szO8zP/fBkZkNmNmJmIyW2BaCkbsO+TtJ8SUslHZL087w/6O7D7j7o7oNdbgtABboKu7sfdvfP3f2UpF9JurratgBUrauwm9nApKfflRRvHAM4xxSOs5vZRkk3SOo3s4OS1kq6wcyWSnJJY5J+UF+L7Vc0jr5+/fpSr180lu3e3mnvZ82alVvr7+/vYSdfVHQPgK/i9eyFYXf3VVMsfqqGXgDUiK/LAkEQdiAIwg4EQdiBIAg7EIT1ctjGzNo7RlTC3r17k/WFCxeWev02D72dq73t3Lkzue7111+frH/yySdd9dQL7j7lX5w9OxAEYQeCIOxAEIQdCIKwA0EQdiAIwg4EwTj7NN199925tU2bNtW67VdffTVZ37hxY25t+/btyXVHRsrdLaxoPPq6667LrS1YsCC57r333ttVT6elbvGdms5ZkrZu3Zqs33zzzV311AuMswPBEXYgCMIOBEHYgSAIOxAEYQeCIOxAEEzZXIG6v6tQNOa7a9eu3Noll1xSatu33nprsn7HHXck66npphctWpRct+z7umHDhtxa0d9ryZIlpbbdRuzZgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiAIrmefptT17KnryatQdG/2iYmJ3NqMGTOS6x49ejRZ7+vrS9bPPz/9VY06/30V3bs9NcZf1Nell16arL/wwgvJepO6vp7dzOaZ2VYze9vM3jKzH2XLLzazF81sX/Y7/a8CQKOmcxj/maQH3H2xpGsk/dDMFkt6SNIWd18oaUv2HEBLFYbd3Q+5+87s8ceS9kqaK2mlpNPfR9wg6a6aegRQgbP6bryZXSbp25L+LGmOux/KSu9LmpOzzpCkoRI9AqjAtM/Gm9mFkp6R9GN3Pz655p2zHVOe8XD3YXcfdPfBUp0CKGVaYTezmeoE/Tfu/my2+LCZDWT1AUlH6mkRQBUKD+OtM+7zlKS97v6LSaXNklZL+ln2+/laOmyJDz74ILf26aefJtedOXNm1e18waxZs7pet7+/v8JOqnXs2LFkPTUcKpW/TfZXzXQ+s/+zpH+RtMfMdmfLHlYn5L8zszWS9kv6Xi0dAqhEYdjd/RVJed/quKnadgDUha/LAkEQdiAIwg4EQdiBIAg7EASXuFbgmmuuSdZfeeWVUq9fdIlrL/8bnqlMb0WX1w4Opr90uX///mQ9KqZsBoIj7EAQhB0IgrADQRB2IAjCDgRB2IEgGGcHvmIYZweCI+xAEIQdCIKwA0EQdiAIwg4EQdiBIAg7EARhB4Ig7EAQhB0IgrADQRB2IAjCDgRB2IEgCsNuZvPMbKuZvW1mb5nZj7Llj5jZuJntzn5ur79dAN0qvHmFmQ1IGnD3nWb2NUk7JN2lznzsf3P3x6e9MW5eAdQu7+YV05mf/ZCkQ9njj81sr6S51bYHoG5n9ZndzC6T9G1Jf84W3W9mb5rZ02bWl7POkJmNmNlIuVYBlDHte9CZ2YWSXpb0H+7+rJnNkfShJJf0U3UO9f+t4DU4jAdqlncYP62wm9lMSb+X9Ad3/8UU9csk/d7dryh4HcIO1KzrG05aZ5rOpyTtnRz07MTdad+VNFq2SQD1mc7Z+GWS/iRpj6RT2eKHJa2StFSdw/gxST/ITualXos9O1CzUofxVSHsQP24bzwQHGEHgiDsQBCEHQiCsANBEHYgCMIOBEHYgSAIOxAEYQeCIOxAEIQdCIKwA0EQdiCIwhtOVuxDSfsnPe/PlrVRW3tra18SvXWryt7+Ma/Q0+vZv7RxsxF3H2ysgYS29tbWviR661aveuMwHgiCsANBNB324Ya3n9LW3tral0Rv3epJb41+ZgfQO03v2QH0CGEHgmgk7GZ2m5n9xczeMbOHmughj5mNmdmebBrqRueny+bQO2Jmo5OWXWxmL5rZvuz3lHPsNdRbK6bxTkwz3uh71/T05z3/zG5mMyT9VdItkg5K2i5plbu/3dNGcpjZmKRBd2/8Cxhmdr2kv0n6r9NTa5nZf0r6yN1/lv2Pss/d/70lvT2is5zGu6be8qYZ/1c1+N5VOf15N5rYs18t6R13f9fdT0raJGllA320nrtvk/TRGYtXStqQPd6gzj+WnsvprRXc/ZC778wefyzp9DTjjb53ib56oomwz5V0YNLzg2rXfO8u6Y9mtsPMhppuZgpzJk2z9b6kOU02M4XCabx76Yxpxlvz3nUz/XlZnKD7smXu/k+SviPph9nhait55zNYm8ZO10mar84cgIck/bzJZrJpxp+R9GN3Pz651uR7N0VfPXnfmgj7uKR5k55/PVvWCu4+nv0+Iuk5dT52tMnh0zPoZr+PNNzP37n7YXf/3N1PSfqVGnzvsmnGn5H0G3d/Nlvc+Hs3VV+9et+aCPt2SQvN7JtmNkvS9yVtbqCPLzGz2dmJE5nZbEkr1L6pqDdLWp09Xi3p+QZ7+YK2TOOdN824Gn7vGp/+3N17/iPpdnXOyP+fpJ800UNOX9+S9Eb281bTvUnaqM5h3afqnNtYI+kSSVsk7ZP0kqSLW9Tbr9WZ2vtNdYI10FBvy9Q5RH9T0u7s5/am37tEXz153/i6LBAEJ+iAIAg7EARhB4Ig7EAQhB0IgrADQRB2IIj/B466i8OZ6ASyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = X_train[82]\n",
    "plt.imshow(np.squeeze(image),cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image.reshape(1,image.shape[0],image.shape[1],image.shape[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image.shape)\n",
    "pred = model.predict([image])\n",
    "print('Predicted number is {}'.format(np.argmax(pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inception v3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "img_path = '/home/vignesh/Desktop/photos/IMG_20191103_193037.jpg'\n",
    "img = image.load_img(img_path, target_size=(224, 224))\n",
    "plt.imshow(img)\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = preprocess_input(x)\n",
    "\n",
    "preds = model.predict(x)\n",
    "# decode the results into a list of tuples (class, description, probability)\n",
    "# (one such list for each sample in the batch)\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2],[1,5],[1,3]])\n",
    "\n",
    "b = np.expand_dims(a,axis=0)\n",
    "\n",
    "print(a)\n",
    "print('----------')\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
